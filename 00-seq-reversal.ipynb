{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import json, itertools\n",
    "from datetime import datetime\n",
    "from random import randint, seed\n",
    "torch.manual_seed(0)\n",
    "seed(0)\n",
    "tqdm.get_lock().locks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a warm-up exercise let's confirm that quadratic-sized recurrent neural network is capable of reverting a simple sequence.\n",
    "Just for the sake of simplicity the sequence will be one-hot encoded and put through a network with 2 recurrent layers of $seq\\_length^2$ neurons and an output linear layer providing a final output with the next sequence item. \n",
    "\n",
    "TODO:\n",
    "- [ ] enable teacher forcing randomization\n",
    "- [ ] perform validation during the training procedure\n",
    "- [ ] improve the code quality\n",
    "- [x] work in batches\n",
    "- [x] check border conditions\n",
    "- [ ] improve progress reporting\n",
    "- [x] try tensorboard output\n",
    "- [x] switch to DVC-based experimentation\n",
    "- [ ] implement sequence padding to enable variable sequence length capability in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 7, 5, 0, 3, 3, 3, 7, 1, 3],\n",
      "        [5, 2, 4, 7, 6, 0, 0, 4, 2, 1],\n",
      "        [6, 7, 7, 6, 0, 1, 5, 1, 5, 0],\n",
      "        [1, 4, 3, 0, 3, 5, 6, 7, 7, 0],\n",
      "        [2, 3, 0, 1, 3, 5, 3, 3, 6, 7],\n",
      "        [0, 1, 1, 1, 7, 0, 7, 2, 4, 7],\n",
      "        [3, 6, 3, 2, 7, 4, 2, 0, 0, 4],\n",
      "        [5, 5, 6, 0, 4, 1, 7, 4, 1, 2],\n",
      "        [2, 7, 0, 1, 1, 7, 1, 1, 3, 6],\n",
      "        [7, 3, 6, 2, 3, 0, 6, 3, 5, 4]])\n",
      "tensor([[3, 1, 7, 3, 3, 3, 0, 5, 7, 4],\n",
      "        [1, 2, 4, 0, 0, 6, 7, 4, 2, 5],\n",
      "        [0, 5, 1, 5, 1, 0, 6, 7, 7, 6],\n",
      "        [0, 7, 7, 6, 5, 3, 0, 3, 4, 1],\n",
      "        [7, 6, 3, 3, 5, 3, 1, 0, 3, 2],\n",
      "        [7, 4, 2, 7, 0, 7, 1, 1, 1, 0],\n",
      "        [4, 0, 0, 2, 4, 7, 2, 3, 6, 3],\n",
      "        [2, 1, 4, 7, 1, 4, 0, 6, 5, 5],\n",
      "        [6, 3, 1, 1, 7, 1, 1, 0, 7, 2],\n",
      "        [4, 5, 3, 6, 0, 3, 2, 6, 3, 7]])\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "with open(\"00-reversal-config.json\", 'r') as ch:\n",
    "    config = json.load(ch)\n",
    "\n",
    "src_sequences = torch.randint(config['VOCAB_SIZE'] - 2, [config['SAMPLES'], config['SEQ_LENGTH']])\n",
    "reversed_sequences = src_sequences.flip(1)\n",
    "print(src_sequences[:10])\n",
    "print(reversed_sequences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sequences_one_hot = torch.zeros((config['SAMPLES'], config['SEQ_LENGTH'], config['VOCAB_SIZE'])).scatter(2, src_sequences.unsqueeze(2), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseEncoder(nn.Module):\n",
    "    def __init__(self, seq_length, vocab_size, rec_layers_count):\n",
    "        super(ReverseEncoder, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.rec_layers_count = rec_layers_count \n",
    "        self.rec_layers = nn.RNN(\n",
    "            input_size = vocab_size, \n",
    "            hidden_size = seq_length**2,\n",
    "            nonlinearity = \"tanh\",\n",
    "            num_layers = rec_layers_count,\n",
    "            batch_first = True\n",
    "        )\n",
    "        \n",
    "    def init_hidden_state(self, input_sequence):\n",
    "        return torch.randn((self.rec_layers_count, input_sequence.size(0), self.seq_length**2))\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        post_recurrent, hidden = self.rec_layers(input_sequence, self.init_hidden_state(input_sequence))\n",
    "\n",
    "        return post_recurrent, hidden\n",
    "    \n",
    "class ReverseDecoder(nn.Module):\n",
    "    def __init__(self, seq_length, vocab_size, rec_layers_count):\n",
    "        super(ReverseDecoder, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.rec_layers_count = rec_layers_count \n",
    "        self.rec_layers = nn.RNN(\n",
    "            input_size = vocab_size, \n",
    "            hidden_size = seq_length**2,\n",
    "            nonlinearity = \"tanh\",\n",
    "            num_layers = rec_layers_count,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.output = nn.Linear(seq_length*seq_length, vocab_size)\n",
    "        \n",
    "    def forward(self, input_sequence, hidden_state):\n",
    "        post_recurrent, hidden = self.rec_layers(input_sequence, hidden_state)\n",
    "        item_probs = F.log_softmax(self.output(post_recurrent), dim=2)\n",
    "        return item_probs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model = ReverseEncoder(config['SEQ_LENGTH'], config['VOCAB_SIZE'], 2)\n",
    "dec_model = ReverseDecoder(config['SEQ_LENGTH'], config['VOCAB_SIZE'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReverseEncoder(\n",
      "  (rec_layers): RNN(10, 100, num_layers=2, batch_first=True)\n",
      ")\n",
      "ReverseDecoder(\n",
      "  (rec_layers): RNN(10, 100, num_layers=2, batch_first=True)\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(enc_model)\n",
    "print(dec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(enc_model.parameters()) + list(dec_model.parameters()), \n",
    "    lr = config['LEARNING_RATE']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS = torch.tensor(config['VOCAB_SIZE'] - 2)\n",
    "EOS = torch.tensor(config['VOCAB_SIZE'] - 1)\n",
    "SOS_filler = torch.cat((torch.zeros(config['VOCAB_SIZE'] - 2), torch.tensor([1.0, 0.0])))\n",
    "EOS_filler = torch.cat((torch.zeros(config['VOCAB_SIZE'] - 1), torch.tensor([1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 42.45it/s, epoch=1/75, mean_loss=22.9, last_loss=22.5]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.84it/s, epoch=2/75, mean_loss=21.9, last_loss=21.4]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.73it/s, epoch=3/75, mean_loss=21.1, last_loss=21]  \n",
      "100%|██████████| 32/32 [00:00<00:00, 45.95it/s, epoch=4/75, mean_loss=20.9, last_loss=20.9]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.98it/s, epoch=5/75, mean_loss=20.8, last_loss=20.8]\n",
      "100%|██████████| 32/32 [00:00<00:00, 46.36it/s, epoch=6/75, mean_loss=20.8, last_loss=20.8]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.91it/s, epoch=7/75, mean_loss=20.7, last_loss=20.7]\n",
      "100%|██████████| 32/32 [00:00<00:00, 46.22it/s, epoch=8/75, mean_loss=20.7, last_loss=20.6]\n",
      "100%|██████████| 32/32 [00:00<00:00, 46.31it/s, epoch=9/75, mean_loss=20.6, last_loss=20.6]\n",
      "100%|██████████| 32/32 [00:00<00:00, 46.11it/s, epoch=10/75, mean_loss=20.5, last_loss=20.4]\n",
      "100%|██████████| 32/32 [00:00<00:00, 46.05it/s, epoch=11/75, mean_loss=20.3, last_loss=20.2]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.79it/s, epoch=12/75, mean_loss=20, last_loss=19.9]  \n",
      "100%|██████████| 32/32 [00:00<00:00, 45.89it/s, epoch=13/75, mean_loss=19.4, last_loss=19.2]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.67it/s, epoch=14/75, mean_loss=18.6, last_loss=18.3]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.58it/s, epoch=15/75, mean_loss=17.8, last_loss=17.3]\n",
      "100%|██████████| 32/32 [00:00<00:00, 44.90it/s, epoch=16/75, mean_loss=16.8, last_loss=16.4]\n",
      "100%|██████████| 32/32 [00:00<00:00, 46.29it/s, epoch=17/75, mean_loss=16, last_loss=15.5]  \n",
      "100%|██████████| 32/32 [00:00<00:00, 45.88it/s, epoch=18/75, mean_loss=15.3, last_loss=14.7]\n",
      "100%|██████████| 32/32 [00:00<00:00, 44.87it/s, epoch=19/75, mean_loss=14.6, last_loss=14]  \n",
      "100%|██████████| 32/32 [00:00<00:00, 44.78it/s, epoch=20/75, mean_loss=13.8, last_loss=13.2]\n",
      "100%|██████████| 32/32 [00:00<00:00, 44.92it/s, epoch=21/75, mean_loss=13, last_loss=12.2]  \n",
      "100%|██████████| 32/32 [00:00<00:00, 45.37it/s, epoch=22/75, mean_loss=12.3, last_loss=11.4]\n",
      "100%|██████████| 32/32 [00:00<00:00, 44.67it/s, epoch=23/75, mean_loss=11.4, last_loss=10.5]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.07it/s, epoch=24/75, mean_loss=10.7, last_loss=9.77]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.20it/s, epoch=25/75, mean_loss=9.91, last_loss=9.06]\n",
      "100%|██████████| 32/32 [00:00<00:00, 44.09it/s, epoch=26/75, mean_loss=9.21, last_loss=8.31]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.45it/s, epoch=27/75, mean_loss=8.53, last_loss=7.6] \n",
      "100%|██████████| 32/32 [00:00<00:00, 45.48it/s, epoch=28/75, mean_loss=7.89, last_loss=7.01]\n",
      "100%|██████████| 32/32 [00:00<00:00, 44.77it/s, epoch=29/75, mean_loss=7.28, last_loss=6.26]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.14it/s, epoch=30/75, mean_loss=6.73, last_loss=5.8] \n",
      "100%|██████████| 32/32 [00:00<00:00, 45.28it/s, epoch=31/75, mean_loss=6.22, last_loss=5.33]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.44it/s, epoch=32/75, mean_loss=5.76, last_loss=4.98]\n",
      "100%|██████████| 32/32 [00:00<00:00, 44.25it/s, epoch=33/75, mean_loss=5.34, last_loss=4.52]\n",
      "100%|██████████| 32/32 [00:00<00:00, 44.94it/s, epoch=34/75, mean_loss=4.93, last_loss=4.22]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.58it/s, epoch=35/75, mean_loss=4.57, last_loss=3.95]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.00it/s, epoch=36/75, mean_loss=4.23, last_loss=3.64]\n",
      "100%|██████████| 32/32 [00:00<00:00, 44.82it/s, epoch=37/75, mean_loss=3.91, last_loss=3.38]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.55it/s, epoch=38/75, mean_loss=3.62, last_loss=3.05]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.27it/s, epoch=39/75, mean_loss=3.35, last_loss=2.81]\n",
      "100%|██████████| 32/32 [00:00<00:00, 44.78it/s, epoch=40/75, mean_loss=3.11, last_loss=2.67]\n",
      "100%|██████████| 32/32 [00:00<00:00, 45.06it/s, epoch=41/75, mean_loss=2.89, last_loss=2.45]\n",
      "100%|██████████| 32/32 [00:00<00:00, 43.40it/s, epoch=42/75, mean_loss=2.67, last_loss=2.25]\n",
      "100%|██████████| 32/32 [00:00<00:00, 43.10it/s, epoch=43/75, mean_loss=2.47, last_loss=2.06]\n",
      "100%|██████████| 32/32 [00:00<00:00, 43.30it/s, epoch=44/75, mean_loss=2.29, last_loss=1.85]\n",
      "100%|██████████| 32/32 [00:00<00:00, 43.29it/s, epoch=45/75, mean_loss=2.13, last_loss=1.82]\n",
      "100%|██████████| 32/32 [00:00<00:00, 42.63it/s, epoch=46/75, mean_loss=1.98, last_loss=1.6] \n",
      "100%|██████████| 32/32 [00:00<00:00, 42.52it/s, epoch=47/75, mean_loss=1.85, last_loss=1.52]\n",
      "100%|██████████| 32/32 [00:00<00:00, 42.22it/s, epoch=48/75, mean_loss=1.73, last_loss=1.4] \n",
      "100%|██████████| 32/32 [00:00<00:00, 42.63it/s, epoch=49/75, mean_loss=1.61, last_loss=1.3] \n",
      "100%|██████████| 32/32 [00:00<00:00, 42.87it/s, epoch=50/75, mean_loss=1.51, last_loss=1.24]\n",
      "100%|██████████| 32/32 [00:00<00:00, 42.36it/s, epoch=51/75, mean_loss=1.42, last_loss=1.17]\n",
      "100%|██████████| 32/32 [00:00<00:00, 42.94it/s, epoch=52/75, mean_loss=1.34, last_loss=1.07]\n",
      "100%|██████████| 32/32 [00:00<00:00, 42.06it/s, epoch=53/75, mean_loss=1.26, last_loss=0.943]\n",
      "100%|██████████| 32/32 [00:00<00:00, 42.66it/s, epoch=54/75, mean_loss=1.18, last_loss=0.932]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.45it/s, epoch=55/75, mean_loss=1.12, last_loss=0.878]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.41it/s, epoch=56/75, mean_loss=1.06, last_loss=0.822]\n",
      "100%|██████████| 32/32 [00:00<00:00, 40.49it/s, epoch=57/75, mean_loss=1, last_loss=0.771]   \n",
      "100%|██████████| 32/32 [00:00<00:00, 41.11it/s, epoch=58/75, mean_loss=0.949, last_loss=0.753]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.51it/s, epoch=59/75, mean_loss=0.899, last_loss=0.697]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.90it/s, epoch=60/75, mean_loss=0.855, last_loss=0.663]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.48it/s, epoch=61/75, mean_loss=0.806, last_loss=0.617]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.96it/s, epoch=62/75, mean_loss=0.772, last_loss=0.62] \n",
      "100%|██████████| 32/32 [00:00<00:00, 41.67it/s, epoch=63/75, mean_loss=0.735, last_loss=0.572]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.31it/s, epoch=64/75, mean_loss=0.695, last_loss=0.523]\n",
      "100%|██████████| 32/32 [00:00<00:00, 42.14it/s, epoch=65/75, mean_loss=0.666, last_loss=0.504]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.31it/s, epoch=66/75, mean_loss=0.628, last_loss=0.5]  \n",
      "100%|██████████| 32/32 [00:00<00:00, 41.24it/s, epoch=67/75, mean_loss=0.602, last_loss=0.446]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.42it/s, epoch=68/75, mean_loss=0.576, last_loss=0.429]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.47it/s, epoch=69/75, mean_loss=0.55, last_loss=0.413] \n",
      "100%|██████████| 32/32 [00:00<00:00, 41.53it/s, epoch=70/75, mean_loss=0.521, last_loss=0.414]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.51it/s, epoch=71/75, mean_loss=0.503, last_loss=0.368]\n",
      "100%|██████████| 32/32 [00:00<00:00, 40.86it/s, epoch=72/75, mean_loss=0.479, last_loss=0.352]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.45it/s, epoch=73/75, mean_loss=0.457, last_loss=0.341]\n",
      "100%|██████████| 32/32 [00:00<00:00, 41.62it/s, epoch=74/75, mean_loss=0.44, last_loss=0.33]  \n",
      "100%|██████████| 32/32 [00:00<00:00, 41.22it/s, epoch=75/75, mean_loss=0.423, last_loss=0.321]\n"
     ]
    }
   ],
   "source": [
    "losses = {}\n",
    "tstart = datetime.now()\n",
    "for epoch in range(config['EPOCHS']):\n",
    "    losses[epoch] = []\n",
    "    chunked_X = chunks(src_sequences_one_hot, config['BATCH_SIZE'])\n",
    "    chunked_y = chunks(reversed_sequences, config['BATCH_SIZE'])\n",
    "    input_chunks = zip(chunked_X, chunked_y)\n",
    "    with tqdm(list(input_chunks)) as cit:\n",
    "        batch_element = 0\n",
    "        for chunk_X, chunk_y in cit:\n",
    "            loss = 0\n",
    "            y_eos = EOS.unsqueeze(0).repeat(chunk_y.size(0)).view(-1,1)\n",
    "            yss = torch.cat((chunk_y, y_eos), dim=1).unsqueeze(1)\n",
    "            x_sos = SOS_filler.unsqueeze(0).repeat(chunk_X.size(0), 1).unsqueeze(1)\n",
    "            single_batch_result_out, hidden = enc_model(chunk_X)\n",
    "            Xss = torch.cat((x_sos, chunk_X.squeeze(0)), dim=1)\n",
    "\n",
    "            for slice_id in range(chunk_y.size(1)):\n",
    "                y_pred, hidden = dec_model(Xss[:, slice_id].unsqueeze(1), hidden)\n",
    "                cur_loss = loss_function(y_pred.squeeze(1), yss[:, :, slice_id].squeeze(1))\n",
    "                loss += cur_loss\n",
    "            enc_model.zero_grad()\n",
    "            dec_model.zero_grad()\n",
    "            loss.backward()\n",
    "            losses[epoch].append(loss.tolist())\n",
    "            optimizer.step()\n",
    "            cit.set_postfix({\n",
    "                'epoch': f\"{epoch+1}/{config['EPOCHS']}\", \n",
    "                'mean_loss': sum(losses[epoch])/len(losses[epoch]),\n",
    "                'last_loss': losses[epoch][-1]\n",
    "            })\n",
    "\n",
    "tend = datetime.now()\n",
    "tdiff = tend - tstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_sequence(seq, model):\n",
    "    seq_one_hot = torch.zeros((1, config['SEQ_LENGTH'], config['VOCAB_SIZE'])).scatter(2, torch.tensor(seq).unsqueeze(0).unsqueeze(-1), 1.0)\n",
    "    (_, hidden) = model[0](seq_one_hot)\n",
    "    result = []\n",
    "    out, hidden = model[1](SOS_filler.unsqueeze(0).unsqueeze(0), hidden)\n",
    "    result.append(torch.argmax(out, dim=2).squeeze(0).squeeze(0).tolist())\n",
    "    for seq_char in seq_one_hot.squeeze(0):\n",
    "        out, hidden = model[1](seq_char.unsqueeze(0).unsqueeze(0), hidden)\n",
    "        result.append(torch.argmax(out, dim=2).squeeze(0).squeeze(0).tolist())\n",
    "    return result[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def const_generator(character_to_generate):\n",
    "    while True:\n",
    "        yield character_to_generate\n",
    "        \n",
    "def random_generator(cap = config['VOCAB_SIZE'] - 2):\n",
    "    while True:\n",
    "        yield randint(0, cap)\n",
    "        \n",
    "def random_generator_fixed_length(character_to_generate, randomized_length, cap = config['VOCAB_SIZE'] - 2):\n",
    "    produced_chars = 0\n",
    "    while True:\n",
    "        if produced_chars < randomized_length:\n",
    "            yield randint(0, cap)\n",
    "        else:\n",
    "            yield character_to_generate\n",
    "        produced_chars += 1\n",
    "\n",
    "def up_stairs_generator(cap = config['VOCAB_SIZE'] - 2, cur_character = 0):\n",
    "    while True:\n",
    "        cur_character += 1\n",
    "        if cur_character == cap + 1:\n",
    "            cur_character = 0\n",
    "        yield cur_character\n",
    "        \n",
    "def down_stairs_generator(cap = config['VOCAB_SIZE'] - 2, cur_character = 0):\n",
    "    while True:\n",
    "        cur_character -= 1\n",
    "        if cur_character < 0:\n",
    "            cur_character = cap\n",
    "        yield cur_character\n",
    "        \n",
    "def two_way_stairs_generator(cap = config['VOCAB_SIZE'] - 2, cur_character = 0):\n",
    "    delta = 1\n",
    "    while True:\n",
    "        cur_character += delta\n",
    "        if cur_character >= cap:\n",
    "            delta = -1\n",
    "            cur_character = cap\n",
    "        if cur_character <= 0:\n",
    "            delta = 1\n",
    "        yield cur_character\n",
    "\n",
    "\n",
    "\n",
    "test_cases = []\n",
    "for el in range(config['VOCAB_SIZE']):\n",
    "    test_cases.append(list(itertools.islice(const_generator(el), config['SEQ_LENGTH'])))\n",
    "    \n",
    "for el in range(config['VOCAB_SIZE']):\n",
    "    test_cases.append(list(itertools.islice(up_stairs_generator(el), config['SEQ_LENGTH'])))\n",
    "    \n",
    "for el in range(config['VOCAB_SIZE']):\n",
    "    test_cases.append(list(itertools.islice(down_stairs_generator(el), config['SEQ_LENGTH'])))\n",
    "        \n",
    "for el in range(config['VOCAB_SIZE']):\n",
    "    test_cases.append(list(itertools.islice(two_way_stairs_generator(el), config['SEQ_LENGTH'])))\n",
    "    \n",
    "for el in range(config['VOCAB_SIZE'] - 2, 0, -1):\n",
    "    test_cases.append(list(itertools.islice(two_way_stairs_generator(el, cur_character = el), config['SEQ_LENGTH'])))\n",
    "    \n",
    "for el in range(config['VOCAB_SIZE']):\n",
    "    test_cases.append(list(itertools.islice(random_generator(), config['SEQ_LENGTH'])))\n",
    "    \n",
    "for el in range(config['VOCAB_SIZE']):\n",
    "    test_cases.append(list(itertools.islice(random_generator_fixed_length(randomized_length = el, character_to_generate = 0), config['SEQ_LENGTH'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "scores = ([SequenceMatcher(a = t, b = reverse_sequence(t, (enc_model, dec_model))[::-1]).ratio() for t in test_cases])\n",
    "score = sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9161764705882351"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./00-seq-reversal-score.json\", 'w') as ch:\n",
    "    config = json.dump({\"score\": score, \"training_time\": tdiff.total_seconds()}, ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
